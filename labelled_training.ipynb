{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-01 19:09:02.726079: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-02-01 19:09:02.762206: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-02-01 19:09:02.762385: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "import os\n",
    "\n",
    "import keras\n",
    "\n",
    "# import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import models\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics.pairwise import euclidean_distances, cosine_similarity\n",
    "# from sklearn.utils import shuffle\n",
    "from sklearn.utils import class_weight\n",
    "# from sklearn.metrics import r2_score\n",
    "# from sklearn.metrics import mean_absolute_error\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import tensorflow.keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization, LSTM\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.callbacks import EarlyStopping, TensorBoard\n",
    "\n",
    "os.environ[\"KERAS_BACKEND\"] = \"tensorflow\"\n",
    "os.environ[\"TF_ENABLE_GPU_GARBAGE_COLLECTION\"] = 'false'\n",
    "os.environ[\"TF_GPU_ALLOCATOR\"] = \"cuda_malloc_async\"\n",
    "\n",
    "physical_devices = tf.config.experimental.list_physical_devices('GPU')\n",
    "tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
    "\n",
    "tensorboard = TensorBoard(log_dir=\"./logs\")\n",
    "\n",
    "DEBUG = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_names = ['timestamp','open','close','high','low','volume','adosc','atr','macd','macd_signal','macd_hist','mfi','upper_band','middle_band','lower_band','rsi','difference_low_high','difference_open_close','target']\n",
    "\n",
    "data_directory = '/home/joren/Coding/cryptodata/Normalized_labelled/'\n",
    "max_df_length = 45000\n",
    "\n",
    "#####################################\n",
    "frame_size = 360\n",
    "layers = 14\n",
    "layer_sizes = [256]*layers\n",
    "dropouts = [0.1]*layers\n",
    "batchnormalizations = [1]*layers\n",
    "learning_rate = 0.0001\n",
    "optimizer = Adam(learning_rate)\n",
    "\n",
    "# class_weights = {0: 1,\n",
    "#                 1: 50.,\n",
    "#                 2: 50.}\n",
    "\n",
    "#####################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "field_info = [\n",
    "    { \"type\": np.uint64, \"count\": 1 },\n",
    "    { \"type\": np.double, \"count\": 17 },\n",
    "    { \"type\": np.int64, \"count\": 1 }\n",
    "]\n",
    "BYTES_EIGHT = 8\n",
    "\n",
    "def read_bin_full_file(file):\n",
    "    f = open(file, 'rb')\n",
    "    b = f.read(-1)\n",
    "\n",
    "    BYTES_TO_READ = 0\n",
    "    for field in field_info:\n",
    "        BYTES_TO_READ += BYTES_EIGHT * field[\"count\"]\n",
    "\n",
    "    data = []\n",
    "    BYTES_READ = 0\n",
    "    for i in range(0, int(os.path.getsize(file) / BYTES_TO_READ)):\n",
    "        row = []\n",
    "\n",
    "        for idx, field in enumerate(field_info):\n",
    "            row += np.frombuffer(b, dtype=field[\"type\"], count=field[\"count\"], offset=BYTES_READ).tolist()\n",
    "\n",
    "            BYTES_READ += BYTES_EIGHT * field[\"count\"]\n",
    "\n",
    "        data.append(row)\n",
    "    return np.array(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_file():\n",
    "    filenames = []\n",
    "    for file in os.listdir(data_directory):\n",
    "        filename = os.fsdecode(file)\n",
    "        filenames.append(filename)\n",
    "        \n",
    "    randomname = filenames[random.randint(0, len(filenames)-1)]\n",
    "    if randomname.endswith(\".bin\"): \n",
    "        print(f\"reading file: {os.path.join(data_directory, randomname)}\")\n",
    "        return os.path.join(data_directory, randomname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(tf.keras.Model):\n",
    "    def __init__(self, n_actions, feature_size, layers = 2, layer_sizes = [128, 128], dropouts = [0.1, 0], batchnormalizations = [0, 0], optimizer='adam'):\n",
    "        super().__init__()\n",
    "        self._n_actions = n_actions\n",
    "        self._feature_size = feature_size\n",
    "        self._frame_size = frame_size\n",
    "\n",
    "        self._model = self.create_model(layers, layer_sizes, dropouts, batchnormalizations, optimizer)\n",
    "    \n",
    "    def create_model(self, layers, layer_sizes, dropouts, batchnormalizations, optimizer):\n",
    "        model = Sequential()\n",
    "\n",
    "        for i in range(0, layers):\n",
    "            if i == 0:\n",
    "                model.add(LSTM(units=layer_sizes[i], return_sequences = True, input_shape = (self._frame_size, self._feature_size)))\n",
    "            elif i == layers:\n",
    "                model.add(LSTM(units=layer_sizes[i]))\n",
    "            elif i >= len(layer_sizes):\n",
    "                model.add(LSTM(units=layer_sizes[0], return_sequences = True))\n",
    "            else:\n",
    "                model.add(LSTM(units=layer_sizes[i], return_sequences = True))\n",
    "            \n",
    "            if i < len(dropouts) and dropouts[i] > 0:\n",
    "                model.add(Dropout(dropouts[i]))\n",
    "            if i < len(batchnormalizations) and batchnormalizations[i] == 1:\n",
    "                model.add(BatchNormalization()) \n",
    "\n",
    "        model.add(LSTM(units=layer_sizes[0], return_sequences = False))\n",
    "        \n",
    "        model.add(Dense(units=128, activation='relu'))\n",
    "        model.add(Dense(units=self._n_actions, activation='softmax'))\n",
    "        \n",
    "        model.compile(optimizer=optimizer, loss=keras.losses.categorical_crossentropy, metrics=['accuracy'])\n",
    "\n",
    "        if DEBUG:\n",
    "            print(model.summary())\n",
    "        return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train-test split\n",
    "def df_split(df):  \n",
    "    X = df.drop(columns=['timestamp','target'], axis=0).to_numpy()\n",
    "    Y = df['target'].to_numpy()\n",
    "\n",
    "    X_train_raw, X_test_raw, y_train_raw, y_test_raw = train_test_split(X, Y, test_size=0.25, shuffle=False)\n",
    "\n",
    "    y_train_raw = to_categorical(y_train_raw, 3).tolist()\n",
    "    y_test_raw = to_categorical(y_test_raw, 3).tolist()\n",
    "\n",
    "    X_train = []\n",
    "    y_train = []\n",
    "    # y_train_weights = []\n",
    "    for i in range(frame_size, X_train_raw.shape[0]): #frame size up to size of array\n",
    "        X_train.append(X_train_raw[i-frame_size:i])\n",
    "        # y_train.append(y_train_raw[i-frame_size:i]) # dit wil ik dus graag veranderen naar y_train_raw[i] zodat we enkel op het einde de output hebben\n",
    "        y_train.append(y_train_raw[i])\n",
    "    X_train, y_train = np.array(X_train), np.array(y_train)\n",
    "    # X_train = np.reshape(X_train, (X_train.shape[0], X_train.shape[1], X_train.shape[2], 1))\n",
    "\n",
    "\n",
    "    X_test = []\n",
    "    y_test = []\n",
    "    for i in range(frame_size, X_test_raw.shape[0]): #frame size up to size of array\n",
    "        X_test.append(X_test_raw[i-frame_size:i])\n",
    "        # y_test.append(y_test_raw[i-frame_size:i])\n",
    "        y_test.append(y_test_raw[i])\n",
    "    X_test, y_test = np.array(X_test), np.array(y_test)\n",
    "\n",
    "    if DEBUG:\n",
    "        print(f\"\"\"\n",
    "        X_train shape: {X_train.shape}\n",
    "        y_train shape: {y_train.shape}\n",
    "        \"\"\")\n",
    "\n",
    "        print(f\"\"\"\n",
    "        X_train[0] shape: {X_train[0].shape}\n",
    "        y_train[0] shape: {y_train[0].shape}\n",
    "        \"\"\")\n",
    "\n",
    "        print(f\"X_train[0]: {X_train[0]}\")\n",
    "        print(f\"y_train[0]: {y_train[0]}\")\n",
    "\n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-01 19:09:04.040451: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-02-01 19:09:04.041467: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-02-01 19:09:04.041694: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-02-01 19:09:04.041854: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-02-01 19:09:04.540568: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-02-01 19:09:04.540783: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-02-01 19:09:04.540921: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-02-01 19:09:04.541020: I tensorflow/core/common_runtime/gpu/gpu_process_state.cc:214] Using CUDA malloc Async allocator for GPU: 0\n",
      "2022-02-01 19:09:04.541114: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 4551 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 2060, pci bus id: 0000:01:00.0, compute capability: 7.5\n"
     ]
    }
   ],
   "source": [
    "# model initialization\n",
    "dqn = DQN(3, 17, layers, layer_sizes, dropouts, batchnormalizations, optimizer)\n",
    "# dqn._model = models.load_model('./models/model_small_15')\n",
    "\n",
    "def test_accuracy(model):\n",
    "    y_pred_raw = np.array(model.predict(X_test))\n",
    "\n",
    "    y_pred = np.argmax(y_pred_raw, axis=-1)\n",
    "    y_pred = y_pred.flatten()\n",
    "    y_test_2 = np.argmax(y_test, axis=-1)\n",
    "    y_test_2 = y_test_2.flatten()\n",
    "\n",
    "    # print(y_pred_raw.shape)\n",
    "    # print(y_test.shape)\n",
    "\n",
    "    # print(y_pred_raw[0])\n",
    "    # print(y_test[0])\n",
    "\n",
    "    # y_pred = y_pred_raw\n",
    "    # y_test_2 = y_test\n",
    "\n",
    "    print(f\"\"\"\n",
    "    Class. report:\n",
    "    {classification_report(y_test_2, y_pred)}\n",
    "    \"\"\")\n",
    "\n",
    "    cf = confusion_matrix(y_test_2, y_pred)\n",
    "\n",
    "    print(cf)\n",
    "    print(accuracy_score(y_test_2, y_pred) * 100) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading file: /home/joren/Coding/cryptodata/Normalized_labelled/XVGUSDT.bin\n",
      "33144\n",
      "123\n",
      "123\n",
      "reading file: /home/joren/Coding/cryptodata/Normalized_labelled/STORJUSDT.bin\n",
      "32994\n",
      "198\n",
      "198\n",
      "reading file: /home/joren/Coding/cryptodata/Normalized_labelled/KAVAUSDT.bin\n",
      "33104\n",
      "143\n",
      "143\n",
      "reading file: /home/joren/Coding/cryptodata/Normalized_labelled/TVKUSDT.bin\n",
      "33080\n",
      "155\n",
      "155\n",
      "reading file: /home/joren/Coding/cryptodata/Normalized_labelled/SNXUSDT.bin\n",
      "32968\n",
      "211\n",
      "211\n",
      "{0: 0.33760009706381944, 1: 52.74881516587678, 2: 52.74881516587678}\n",
      "Epoch 1/3\n",
      "522/522 [==============================] - 310s 592ms/step - loss: 1.1208 - accuracy: 0.3062\n",
      "Epoch 2/3\n",
      "522/522 [==============================] - 309s 593ms/step - loss: 1.1162 - accuracy: 0.3333\n",
      "Epoch 3/3\n",
      "522/522 [==============================] - 310s 594ms/step - loss: 1.1084 - accuracy: 0.3184\n",
      "\n",
      "    Class. report:\n",
      "                  precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.19      0.32     10807\n",
      "           1       0.00      0.74      0.01        42\n",
      "           2       0.01      0.07      0.01        41\n",
      "\n",
      "    accuracy                           0.19     10890\n",
      "   macro avg       0.33      0.33      0.11     10890\n",
      "weighted avg       0.98      0.19      0.31     10890\n",
      "\n",
      "    \n",
      "[[2035 8195  577]\n",
      " [   7   31    4]\n",
      " [   9   29    3]]\n",
      "18.999081726354454\n",
      "reading file: /home/joren/Coding/cryptodata/Normalized_labelled/ETCUSDT.bin\n",
      "33217\n",
      "87\n",
      "86\n",
      "reading file: /home/joren/Coding/cryptodata/Normalized_labelled/CELOUSDT.bin\n",
      "32866\n",
      "262\n",
      "262\n",
      "{0: 0.33864784275543114, 1: 42.48091603053435, 2: 42.48091603053435}\n",
      "Epoch 1/3\n",
      "522/522 [==============================] - 311s 594ms/step - loss: 1.1135 - accuracy: 0.4514\n",
      "Epoch 2/3\n",
      "522/522 [==============================] - 310s 594ms/step - loss: 1.1023 - accuracy: 0.3807\n",
      "Epoch 3/3\n",
      "522/522 [==============================] - 309s 592ms/step - loss: 1.1030 - accuracy: 0.3702\n",
      "\n",
      "    Class. report:\n",
      "                  precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.17      0.29     10748\n",
      "           1       0.01      0.85      0.02        71\n",
      "           2       0.01      0.20      0.02        71\n",
      "\n",
      "    accuracy                           0.17     10890\n",
      "   macro avg       0.34      0.40      0.11     10890\n",
      "weighted avg       0.98      0.17      0.28     10890\n",
      "\n",
      "    \n",
      "[[1797 7441 1510]\n",
      " [   7   60    4]\n",
      " [   6   51   14]]\n",
      "17.180899908172638\n",
      "reading file: /home/joren/Coding/cryptodata/Normalized_labelled/MCOUSDT.bin\n",
      "33186\n",
      "102\n",
      "102\n",
      "reading file: /home/joren/Coding/cryptodata/Normalized_labelled/BTCSTUSDT.bin\n",
      "32817\n",
      "286\n",
      "287\n",
      "{0: 0.3391534875217113, 1: 38.91608391608391, 2: 38.78048780487805}\n",
      "Epoch 1/3\n",
      "522/522 [==============================] - 310s 593ms/step - loss: 1.1022 - accuracy: 0.4634\n",
      "Epoch 2/3\n",
      "522/522 [==============================] - 310s 594ms/step - loss: 1.1003 - accuracy: 0.3844\n",
      "Epoch 3/3\n",
      "522/522 [==============================] - 310s 594ms/step - loss: 1.0994 - accuracy: 0.5001\n",
      "\n",
      "    Class. report:\n",
      "                  precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.89      0.94     10796\n",
      "           1       0.01      0.04      0.02        47\n",
      "           2       0.01      0.15      0.01        47\n",
      "\n",
      "    accuracy                           0.88     10890\n",
      "   macro avg       0.34      0.36      0.32     10890\n",
      "weighted avg       0.98      0.88      0.93     10890\n",
      "\n",
      "    \n",
      "[[9561  147 1088]\n",
      " [  35    2   10]\n",
      " [  38    2    7]]\n",
      "87.87878787878788\n",
      "reading file: /home/joren/Coding/cryptodata/Normalized_labelled/REPUSDT.bin\n",
      "33221\n",
      "85\n",
      "84\n",
      "reading file: /home/joren/Coding/cryptodata/Normalized_labelled/AKROUSDT.bin\n",
      "32856\n",
      "267\n",
      "267\n",
      "{0: 0.3387509130752374, 1: 41.68539325842696, 2: 41.68539325842696}\n",
      "Epoch 1/3\n",
      "522/522 [==============================] - 310s 593ms/step - loss: 1.1033 - accuracy: 0.3866\n",
      "Epoch 2/3\n",
      "522/522 [==============================] - 309s 592ms/step - loss: 1.1031 - accuracy: 0.5076\n",
      "Epoch 3/3\n",
      "522/522 [==============================] - 311s 595ms/step - loss: 1.1031 - accuracy: 0.4431\n",
      "\n",
      "    Class. report:\n",
      "                  precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.08      0.15     10757\n",
      "           1       0.01      0.23      0.01        66\n",
      "           2       0.01      0.63      0.01        67\n",
      "\n",
      "    accuracy                           0.08     10890\n",
      "   macro avg       0.33      0.31      0.06     10890\n",
      "weighted avg       0.98      0.08      0.15     10890\n",
      "\n",
      "    \n",
      "[[ 868 2637 7252]\n",
      " [   7   15   44]\n",
      " [   3   22   42]]\n",
      "8.494031221303947\n",
      "reading file: /home/joren/Coding/cryptodata/Normalized_labelled/XLMUSDT.bin\n",
      "33274\n",
      "58\n",
      "58\n",
      "reading file: /home/joren/Coding/cryptodata/Normalized_labelled/IRISUSDT.bin\n",
      "32830\n",
      "280\n",
      "280\n",
      "{0: 0.3390191897654584, 1: 39.75, 2: 39.75}\n"
     ]
    }
   ],
   "source": [
    "for i in range(100000):\n",
    "    file = random_file()\n",
    "    data = read_bin_full_file(file)\n",
    "\n",
    "    df = pd.DataFrame(data, columns=column_names) # variable from cell 1\n",
    "    if len(df) < 5000:\n",
    "        continue\n",
    "    df.fillna(0, inplace=True)\n",
    "    # if df.isnull().values.any():\n",
    "    #     print('nan values found')\n",
    "    #     continue\n",
    "\n",
    "    df = df.iloc[240:]\n",
    "\n",
    "    if len(df) > max_df_length:\n",
    "        randstart = random.randint(0, len(df)-max_df_length)\n",
    "        df = df.iloc[randstart:randstart+max_df_length]\n",
    "\n",
    "    X_train, X_test, y_train, y_test = df_split(df)\n",
    "\n",
    "    # print(X_train.shape)\n",
    "    # print(y_train.shape)\n",
    "\n",
    "    y_train_list = np.argmax(y_train, axis=-1)\n",
    "    next_file = False\n",
    "    for i in [0,1,2]:\n",
    "        print(y_train_list.tolist().count(i))\n",
    "        if y_train_list.tolist().count(i) < 200:\n",
    "            next_file = True\n",
    "    if next_file:\n",
    "        continue\n",
    "\n",
    "    class_weights = dict(enumerate(class_weight.compute_class_weight( class_weight='balanced', classes=[0,1,2], y = y_train_list )))\n",
    "    print(class_weights)\n",
    "\n",
    "    dqn._model.fit(X_train, y_train, epochs = 3, batch_size = 64, callbacks=[tensorboard], class_weight=class_weights)\n",
    "    # if i > 10:\n",
    "    #     dqn._model.save(f'models/model_small_10_{i}')\n",
    "    test_accuracy(dqn._model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm (LSTM)                 (None, 360, 256)          280576    \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 360, 256)          0         \n",
      "                                                                 \n",
      " batch_normalization (BatchN  (None, 360, 256)         1024      \n",
      " ormalization)                                                   \n",
      "                                                                 \n",
      " lstm_1 (LSTM)               (None, 360, 256)          525312    \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 360, 256)          0         \n",
      "                                                                 \n",
      " batch_normalization_1 (Batc  (None, 360, 256)         1024      \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " lstm_2 (LSTM)               (None, 360, 256)          525312    \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 360, 256)          0         \n",
      "                                                                 \n",
      " batch_normalization_2 (Batc  (None, 360, 256)         1024      \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " lstm_3 (LSTM)               (None, 360, 256)          525312    \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, 360, 256)          0         \n",
      "                                                                 \n",
      " batch_normalization_3 (Batc  (None, 360, 256)         1024      \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " lstm_4 (LSTM)               (None, 360, 256)          525312    \n",
      "                                                                 \n",
      " dropout_4 (Dropout)         (None, 360, 256)          0         \n",
      "                                                                 \n",
      " batch_normalization_4 (Batc  (None, 360, 256)         1024      \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " lstm_5 (LSTM)               (None, 360, 256)          525312    \n",
      "                                                                 \n",
      " dropout_5 (Dropout)         (None, 360, 256)          0         \n",
      "                                                                 \n",
      " batch_normalization_5 (Batc  (None, 360, 256)         1024      \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " lstm_6 (LSTM)               (None, 360, 256)          525312    \n",
      "                                                                 \n",
      " dropout_6 (Dropout)         (None, 360, 256)          0         \n",
      "                                                                 \n",
      " batch_normalization_6 (Batc  (None, 360, 256)         1024      \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " lstm_7 (LSTM)               (None, 360, 256)          525312    \n",
      "                                                                 \n",
      " dropout_7 (Dropout)         (None, 360, 256)          0         \n",
      "                                                                 \n",
      " batch_normalization_7 (Batc  (None, 360, 256)         1024      \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " lstm_8 (LSTM)               (None, 360, 256)          525312    \n",
      "                                                                 \n",
      " dropout_8 (Dropout)         (None, 360, 256)          0         \n",
      "                                                                 \n",
      " batch_normalization_8 (Batc  (None, 360, 256)         1024      \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " lstm_9 (LSTM)               (None, 360, 256)          525312    \n",
      "                                                                 \n",
      " dropout_9 (Dropout)         (None, 360, 256)          0         \n",
      "                                                                 \n",
      " batch_normalization_9 (Batc  (None, 360, 256)         1024      \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " lstm_10 (LSTM)              (None, 360, 256)          525312    \n",
      "                                                                 \n",
      " dropout_10 (Dropout)        (None, 360, 256)          0         \n",
      "                                                                 \n",
      " batch_normalization_10 (Bat  (None, 360, 256)         1024      \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " lstm_11 (LSTM)              (None, 360, 256)          525312    \n",
      "                                                                 \n",
      " dropout_11 (Dropout)        (None, 360, 256)          0         \n",
      "                                                                 \n",
      " batch_normalization_11 (Bat  (None, 360, 256)         1024      \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " lstm_12 (LSTM)              (None, 360, 256)          525312    \n",
      "                                                                 \n",
      " dropout_12 (Dropout)        (None, 360, 256)          0         \n",
      "                                                                 \n",
      " batch_normalization_12 (Bat  (None, 360, 256)         1024      \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " lstm_13 (LSTM)              (None, 360, 256)          525312    \n",
      "                                                                 \n",
      " dropout_13 (Dropout)        (None, 360, 256)          0         \n",
      "                                                                 \n",
      " batch_normalization_13 (Bat  (None, 360, 256)         1024      \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " lstm_14 (LSTM)              (None, 256)               525312    \n",
      "                                                                 \n",
      " dense (Dense)               (None, 128)               16512     \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 3)                 387       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 7,682,563\n",
      "Trainable params: 7,675,395\n",
      "Non-trainable params: 7,168\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-01 18:49:57.069629: I tensorflow/stream_executor/stream.cc:4442] [stream=0x560b1e7b6b00,impl=0x560b1e7b7510] INTERNAL: stream did not block host until done; was already in an error state\n",
      "2022-02-01 18:49:57.079244: W tensorflow/python/util/util.cc:368] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\n",
      "WARNING:absl:Found untraced functions such as lstm_cell_15_layer_call_fn, lstm_cell_15_layer_call_and_return_conditional_losses, lstm_cell_16_layer_call_fn, lstm_cell_16_layer_call_and_return_conditional_losses, lstm_cell_17_layer_call_fn while saving (showing 5 of 75). These functions will not be directly callable after loading.\n",
      "2022-02-01 18:50:34.721367: I tensorflow/stream_executor/stream.cc:4442] [stream=0x560b1e7b6b00,impl=0x560b1e7b7510] INTERNAL: stream did not block host until done; was already in an error state\n"
     ]
    },
    {
     "ename": "InternalError",
     "evalue": "Failed copying input tensor from /job:localhost/replica:0/task:0/device:GPU:0 to /job:localhost/replica:0/task:0/device:CPU:0 in order to run Identity: stream did not block host until done; was already in an error state [Op:Identity]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInternalError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_6897/324233602.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# test_accuracy(dqn._model)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdqn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mdqn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'models/model_small_15'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m       \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mraise_from_not_ok_status\u001b[0;34m(e, name)\u001b[0m\n\u001b[1;32m   7105\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mraise_from_not_ok_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   7106\u001b[0m   \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\" name: \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 7107\u001b[0;31m   \u001b[0;32mraise\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   7108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   7109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInternalError\u001b[0m: Failed copying input tensor from /job:localhost/replica:0/task:0/device:GPU:0 to /job:localhost/replica:0/task:0/device:CPU:0 in order to run Identity: stream did not block host until done; was already in an error state [Op:Identity]"
     ]
    }
   ],
   "source": [
    "# save model\n",
    "# test_accuracy(dqn._model)\n",
    "print(dqn._model.summary())\n",
    "dqn._model.save(f'models/model_small_15')\n",
    "dqn._model.save_weights('./models/model_small_15_weights')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
  },
  "kernelspec": {
   "display_name": "Python 3.10.1 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
